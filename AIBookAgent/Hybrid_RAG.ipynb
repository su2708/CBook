{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **하이브리드 검색 시스템**\n",
    "이 코드는 AI 관련 뉴스 기사를 효과적으로 검색하기 위한 하이브리드 검색 시스템을 구현한 것입니다. 벡터 기반 의미론적 검색과 키워드 기반 검색을 결합하여 더 정확한 검색 결과를 제공합니다.\n",
    "\n",
    "##### **주요 기능**\n",
    "1. 문서 처리\n",
    "    - JSON 형식의 뉴스 데이터 로드\n",
    "    - 문서를 청크 단위로 분할\n",
    "    - 벡터 DB 및 키워드 검색용 인덱스 생성\n",
    "\n",
    "2. 하이브리드 검색\n",
    "    - 벡터 기반 의미론적 검색 (FAISS)\n",
    "    - 키워드 기반 검색 (BM25)\n",
    "    - 두 검색 방식의 결과를 가중치를 적용하여 통합\n",
    "\n",
    "3. 데이터 관리\n",
    "    - 벡터 스토어 저장/로드\n",
    "    - 처리된 문서 데이터 저장/로드\n",
    "    - 진행 상황 로깅\n",
    "\n",
    "##### **검색 가중치 설정 가이드**\n",
    "- 의미론적 검색 중심 (semantic_weight=0.7)\n",
    "    - 문맥과 의미를 더 중요하게 고려\n",
    "    - 유사한 주제의 문서도 검색 가능\n",
    "    - 예: \"AI 기술의 미래 전망\" → AI 발전 방향, 기술 트렌드 등 관련 문서 포함\n",
    "\n",
    "- 키워드 검색 중심 (semantic_weight=0.3)\n",
    "    - 정확한 키워드 매칭을 중시\n",
    "    - 특정 용어나 개념이 포함된 문서 우선\n",
    "    - 예: \"삼성전자 AI 칩\" → 정확히 해당 키워드가 포함된 문서 우선\n",
    "\n",
    "- 균형잡힌 검색 (semantic_weight=0.5)\n",
    "    - 두 방식의 장점을 균형있게 활용\n",
    "    - 일반적인 검색에 적합\n",
    "    - 예: \"자율주행 안전\" → 키워드 매칭과 의미적 연관성 모두 고려"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from typing import List, Dict, Tuple\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_EMBEDDING_MODEL = os.getenv(\"OPENAI_EMBEDDING_MODEL\")\n",
    "# JSON_DIR = os.getenv(\"JSON_DIR\")\n",
    "# VECTOR_STORE_PATH = os.getenv(\"VECTOR_STORE_PATH\")\n",
    "# METADATA_PATH = os.getenv(\"METADATA_PATH\")\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "JSON_DIR = \"./books\"\n",
    "VECTOR_STORE_PATH = \"./books_vectorstore\"\n",
    "METADATA_PATH = \"./books_vectorstore/index.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json dir: ./books\n",
      "vector store path: ./books_vectorstore\n",
      "metadata path: ./books_vectorstore/index.pkl\n"
     ]
    }
   ],
   "source": [
    "print(f\"json dir: {JSON_DIR}\")\n",
    "print(f\"vector store path: {VECTOR_STORE_PATH}\")\n",
    "print(f\"metadata path: {METADATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIBooksRAG:\n",
    "    \"\"\"\n",
    "    JSON 데이터를 활용한 Hybrid RAG 시스템\n",
    "    - LangChain의 FAISS 벡터스토어를 이용한 의미론적 검색\n",
    "    - BM25를 이용한 키워드 기반 검색\n",
    "    - 하이브리드 검색 기능 제공\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.vector_store = None\n",
    "        self.metadata = []\n",
    "        self.bm25 = None\n",
    "        self.embeddings = OpenAIEmbeddings(model=OPENAI_EMBEDDING_MODEL, openai_api_key=OPENAI_API_KEY)\n",
    "        self.json_dir = JSON_DIR\n",
    "        self.vector_store_path = VECTOR_STORE_PATH\n",
    "        self.metadata_path = METADATA_PATH\n",
    "\n",
    "    # 1. 임베딩 생성 함수\n",
    "    def get_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"텍스트를 OpenAI 임베딩 모델로 임베딩\"\"\"\n",
    "        return self.embeddings.embed_query(text)\n",
    "\n",
    "    # 2. 목차 파싱 함수\n",
    "    def parse_toc(self, toc_html: str) -> List[Dict[str, List[str]]]:\n",
    "        \"\"\"HTML 형태의 목차 데이터를 파싱하여 계층적 구조로 반환\"\"\"\n",
    "        soup = BeautifulSoup(toc_html, \"html.parser\")\n",
    "        chapters = [b.get_text() for b in soup.find_all(\"b\")]\n",
    "        items = [br.next_sibling.strip() for br in soup.find_all(\"br\") if br.next_sibling]\n",
    "\n",
    "        structured_toc = []\n",
    "        current_chapter = None\n",
    "\n",
    "        for item in items:\n",
    "            if item in chapters:\n",
    "                current_chapter = item\n",
    "                structured_toc.append({\"chapter\": current_chapter, \"items\": []})\n",
    "            elif current_chapter:\n",
    "                structured_toc[-1][\"items\"].append(item)\n",
    "\n",
    "        return structured_toc\n",
    "\n",
    "    # 3. 데이터 임베딩 및 문서 생성 함수\n",
    "    def create_documents(self, book_data: List) -> List[Document]:\n",
    "        \"\"\"책 데이터를 LangChain Document로 변환\"\"\"\n",
    "        documents = []\n",
    "        \n",
    "        for book in book_data:\n",
    "            # 책 기본 정보\n",
    "            documents.append(Document(\n",
    "                page_content=book[\"title\"],\n",
    "                metadata={\n",
    "                    \"type\": \"title\",\n",
    "                    \"title\": book[\"title\"],\n",
    "                    \"author\": book[\"author\"],\n",
    "                    \"pubDate\": book[\"pubDate\"],\n",
    "                    \"categoryName\": book[\"categoryName\"],\n",
    "                    \"toc\": book.get(\"toc\", \"목차 정보 없음\")\n",
    "                }\n",
    "            ))\n",
    "            documents.append(Document(\n",
    "                page_content=book[\"description\"],\n",
    "                metadata={\n",
    "                    \"type\": \"description\",\n",
    "                    \"title\": book[\"title\"],\n",
    "                    \"author\": book[\"author\"],\n",
    "                    \"pubDate\": book[\"pubDate\"],\n",
    "                    \"categoryName\": book[\"categoryName\"],\n",
    "                    \"toc\": book.get(\"toc\", \"목차 정보 없음\")\n",
    "                }\n",
    "            ))\n",
    "\n",
    "            # 목차 정보\n",
    "            toc_html = book[\"toc\"]\n",
    "            structured_toc = self.parse_toc(toc_html)\n",
    "            for chapter in structured_toc:\n",
    "                for item in chapter[\"items\"]:\n",
    "                    item_with_context = f\"{chapter['chapter']} - {item}\"\n",
    "                    documents.append(Document(\n",
    "                        page_content=item_with_context,\n",
    "                        metadata={\n",
    "                            \"type\": \"toc\",\n",
    "                            \"title\": book[\"title\"],\n",
    "                            \"author\": book[\"author\"],\n",
    "                            \"pubDate\": book[\"pubDate\"],\n",
    "                            \"categoryName\": book[\"categoryName\"],\n",
    "                            \"item\": item,\n",
    "                            \"toc\": toc_html\n",
    "                        }\n",
    "                    ))\n",
    "        return documents\n",
    "\n",
    "    # 4. JSON 데이터 로드 함수\n",
    "    def load_json_files(self, directory: str) -> List[Dict]:\n",
    "        \"\"\"지정된 디렉토리에서 모든 JSON 파일을 읽어 책 정보 리스트 반환\"\"\"\n",
    "        data = []\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith(\".json\"):\n",
    "                filepath = os.path.join(directory, filename)\n",
    "                with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data.append(json.load(f))\n",
    "        return data\n",
    "\n",
    "    # 5. FAISS 벡터스토어 생성\n",
    "    def create_vector_store(self):\n",
    "        \"\"\"JSON 데이터를 읽고 FAISS 벡터스토어 생성 및 저장\"\"\"\n",
    "        book_data_list = self.load_json_files(self.json_dir)\n",
    "        documents = []\n",
    "        for book_data in book_data_list:\n",
    "            documents.extend(self.create_documents(book_data))\n",
    "\n",
    "        # LangChain FAISS 벡터스토어 생성\n",
    "        vector_store = FAISS.from_documents(documents, self.embeddings)\n",
    "        vector_store.save_local(self.vector_store_path)\n",
    "        self.vector_store = vector_store\n",
    "        self.metadata = documents\n",
    "        \n",
    "        print(f\"✅ {self.vector_store_path}에 벡터스토어가 생성되었습니다.\")\n",
    "\n",
    "    # 6. FAISS 벡터스토어 로드\n",
    "    def load_vector_store(self):\n",
    "        \"\"\"FAISS 벡터스토어 및 메타데이터 로드\"\"\"\n",
    "        try:\n",
    "            # 벡터스토어 로드 \n",
    "            self.vector_store = FAISS.load_local(\n",
    "                self.vector_store_path,\n",
    "                embeddings=self.embeddings,\n",
    "                allow_dangerous_deserialization=True\n",
    "            )\n",
    "            print(f\"✅ {self.vector_store_path}에서 벡터스토어를 로드했습니다.\")\n",
    "            \n",
    "            # 메타데이터 로드 \n",
    "            with open(self.metadata_path, 'rb') as f:\n",
    "                self.metadata = pickle.load(f)\n",
    "            print(f\"✅ {self.metadata_path}에서 메타데이터를 로드했습니다.\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"❌ 로드 중 오류 발생: {str(e)}\")\n",
    "\n",
    "    # 7. BM25 초기화\n",
    "    def initialize_bm25(self):\n",
    "        \"\"\"BM25 검색 엔진 초기화\"\"\"\n",
    "        if not self.vector_store:\n",
    "            raise ValueError(\"FAISS 벡터스토어가 초기화되지 않았습니다.\")\n",
    "        \n",
    "        # 벡터스토어의 InMemoryDocstore에서 문서 가져오기\n",
    "        documents = self.vector_store.docstore._dict.values()\n",
    "        \n",
    "        # BM25 코퍼스 생성\n",
    "        tokenized_corpus = [\n",
    "            doc.page_content.lower().split() for doc in documents\n",
    "        ]\n",
    "        \n",
    "        self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "        print(\"✅ BM25 검색 엔진 초기화 완료.\")\n",
    "\n",
    "    # 8. Hybrid 검색\n",
    "    def hybrid_search(self, query: str, k: int = 5, semantic_weight: float = 0.5) -> List[Tuple[Dict, float]]:\n",
    "        \"\"\"FAISS 및 BM25를 결합한 하이브리드 검색\"\"\"\n",
    "        # FAISS 검색\n",
    "        faiss_results = self.vector_store.similarity_search_with_score(query, k=k)\n",
    "\n",
    "        # BM25 검색\n",
    "        tokenized_query = query.lower().split()\n",
    "        \n",
    "        # InMemoryDocstore에서 문서 가져오기\n",
    "        documents = list(self.vector_store.docstore._dict.values())\n",
    "        \n",
    "        bm25_scores = self.bm25.get_scores(tokenized_query)\n",
    "        bm25_results = sorted(\n",
    "            [(documents[i], bm25_scores[i]) for i in range(len(documents))],\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )[:k]\n",
    "\n",
    "        # 하이브리드 결합\n",
    "        combined_scores = {}\n",
    "        doc_info = {}  # 문서 정보를 저장할 딕셔너리 \n",
    "        for doc, score in faiss_results:\n",
    "            key = doc.metadata.get('title', 'Unknown Title')\n",
    "            \n",
    "            # FAISS 점수 반영\n",
    "            combined_scores[key] = semantic_weight * (1 - score)\n",
    "            \n",
    "            # 목차 추가 \n",
    "            if key not in doc_info: \n",
    "                doc_info[key] = {\n",
    "                    'title': doc.metadata.get('title', 'Unknown Title'),\n",
    "                    'author': doc.metadata.get('author', 'Unknown Author'),\n",
    "                    'categoryName': doc.metadata.get('categoryName', 'Unknown Category'),\n",
    "                    'pubDate': doc.metadata.get('pubDate', 'Unknown Date'),\n",
    "                    'toc': doc.metadata.get('toc', '목차 정보 없음'),\n",
    "                }\n",
    "        for doc, score in bm25_results:\n",
    "            key = doc.metadata.get('title', 'Unknown Title')\n",
    "            if doc.page_content in combined_scores:\n",
    "                # BM25 점수 반영\n",
    "                combined_scores[key] += (1 - semantic_weight) * score\n",
    "            else:\n",
    "                combined_scores[key] = (1 - semantic_weight) * score\n",
    "                doc_info[key] = {\n",
    "                    'title': doc.metadata.get('title', 'Unknown Title'),\n",
    "                    'author': doc.metadata.get('author', 'Unknown Author'),\n",
    "                    'categoryName': doc.metadata.get('categoryName', 'Unknown Category'),\n",
    "                    'pubDate': doc.metadata.get('pubDate', 'Unknown Date'),\n",
    "                    'toc': doc.metadata.get('toc', '목차 정보 없음'),\n",
    "                }\n",
    "\n",
    "        # 결과 정렬 및 반환 \n",
    "        sorted_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [(doc_info[key], score) for key, score in sorted_results[:k]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIBooksRAG 클래스 초기화\n",
    "rag = AIBooksRAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 벡터스토어 생성\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m rag\u001b[38;5;241m.\u001b[39mcreate_vector_store()\n",
      "Cell \u001b[1;32mIn[50], line 109\u001b[0m, in \u001b[0;36mAIBooksRAG.create_vector_store\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    107\u001b[0m documents \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m book_data \u001b[38;5;129;01min\u001b[39;00m book_data_list:\n\u001b[1;32m--> 109\u001b[0m     documents\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_documents(book_data))\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# LangChain FAISS 벡터스토어 생성\u001b[39;00m\n\u001b[0;32m    112\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_documents(documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings)\n",
      "Cell \u001b[1;32mIn[50], line 74\u001b[0m, in \u001b[0;36mAIBooksRAG.create_documents\u001b[1;34m(self, book_data)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# 목차 정보\u001b[39;00m\n\u001b[0;32m     73\u001b[0m toc_html \u001b[38;5;241m=\u001b[39m book[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 74\u001b[0m structured_toc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_toc(toc_html)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chapter \u001b[38;5;129;01min\u001b[39;00m structured_toc:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m chapter[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "Cell \u001b[1;32mIn[50], line 28\u001b[0m, in \u001b[0;36mAIBooksRAG.parse_toc\u001b[1;34m(self, toc_html)\u001b[0m\n\u001b[0;32m     26\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(toc_html, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m chapters \u001b[38;5;241m=\u001b[39m [b\u001b[38;5;241m.\u001b[39mget_text() \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m---> 28\u001b[0m items \u001b[38;5;241m=\u001b[39m [br\u001b[38;5;241m.\u001b[39mnext_sibling\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m br \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m br\u001b[38;5;241m.\u001b[39mnext_sibling]\n\u001b[0;32m     30\u001b[0m structured_toc \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     31\u001b[0m current_chapter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[50], line 28\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     26\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(toc_html, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m chapters \u001b[38;5;241m=\u001b[39m [b\u001b[38;5;241m.\u001b[39mget_text() \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m---> 28\u001b[0m items \u001b[38;5;241m=\u001b[39m [br\u001b[38;5;241m.\u001b[39mnext_sibling\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m br \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m br\u001b[38;5;241m.\u001b[39mnext_sibling]\n\u001b[0;32m     30\u001b[0m structured_toc \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     31\u001b[0m current_chapter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "# 벡터스토어 생성\n",
    "rag.create_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "❌ 로드 중 오류 발생: too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 124\u001b[0m, in \u001b[0;36mAIBooksRAG.load_vector_store\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;66;03m# 벡터스토어 로드 \u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_store \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mload_local(\n\u001b[0;32m    125\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_store_path,\n\u001b[0;32m    126\u001b[0m         embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings,\n\u001b[0;32m    127\u001b[0m         allow_dangerous_deserialization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    128\u001b[0m     )\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_store_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m에서 벡터스토어를 로드했습니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\SpartaProjects\\Group_Project\\CBook\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1207\u001b[0m, in \u001b[0;36mFAISS.load_local\u001b[1;34m(cls, folder_path, embeddings, index_name, allow_dangerous_deserialization, **kwargs)\u001b[0m\n\u001b[0;32m   1206\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m-> 1207\u001b[0m     (\n\u001b[0;32m   1208\u001b[0m         docstore,\n\u001b[0;32m   1209\u001b[0m         index_to_docstore_id,\n\u001b[0;32m   1210\u001b[0m     ) \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(  \u001b[38;5;66;03m# ignore[pickle]: explicit-opt-in\u001b[39;00m\n\u001b[0;32m   1211\u001b[0m         f\n\u001b[0;32m   1212\u001b[0m     )\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(embeddings, index, docstore, index_to_docstore_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 벡터스토어 로드\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m rag\u001b[38;5;241m.\u001b[39mload_vector_store()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# BM25 초기화\u001b[39;00m\n\u001b[0;32m      5\u001b[0m rag\u001b[38;5;241m.\u001b[39minitialize_bm25()\n",
      "Cell \u001b[1;32mIn[50], line 136\u001b[0m, in \u001b[0;36mAIBooksRAG.load_vector_store\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m에서 메타데이터를 로드했습니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❌ 로드 중 오류 발생: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mException\u001b[0m: ❌ 로드 중 오류 발생: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# 벡터스토어 로드\n",
    "rag.load_vector_store()\n",
    "\n",
    "# BM25 초기화\n",
    "rag.initialize_bm25()\n",
    "\n",
    "# 하이브리드 검색\n",
    "query = \"과학탐구\"\n",
    "results = rag.hybrid_search(query, k=5)\n",
    "print(\"\\n=== Hybrid Search Results ===\")\n",
    "\n",
    "for doc_type, score in results:\n",
    "    print(f\"Type: {doc_type}, Score: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\USER\\\\anaconda3\\\\envs\\\\SpartaProjects\\\\Group_Project\\\\CBook\\\\CBook\\\\AIBookAgent'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CBook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
