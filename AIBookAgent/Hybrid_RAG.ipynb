{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **하이브리드 검색 시스템**\n",
    "이 코드는 AI 관련 뉴스 기사를 효과적으로 검색하기 위한 하이브리드 검색 시스템을 구현한 것입니다. 벡터 기반 의미론적 검색과 키워드 기반 검색을 결합하여 더 정확한 검색 결과를 제공합니다.\n",
    "\n",
    "##### **주요 기능**\n",
    "1. 문서 처리\n",
    "    - JSON 형식의 뉴스 데이터 로드\n",
    "    - 문서를 청크 단위로 분할\n",
    "    - 벡터 DB 및 키워드 검색용 인덱스 생성\n",
    "\n",
    "2. 하이브리드 검색\n",
    "    - 벡터 기반 의미론적 검색 (FAISS)\n",
    "    - 키워드 기반 검색 (BM25)\n",
    "    - 두 검색 방식의 결과를 가중치를 적용하여 통합\n",
    "\n",
    "3. 데이터 관리\n",
    "    - 벡터 스토어 저장/로드\n",
    "    - 처리된 문서 데이터 저장/로드\n",
    "    - 진행 상황 로깅\n",
    "\n",
    "##### **검색 가중치 설정 가이드**\n",
    "- 의미론적 검색 중심 (semantic_weight=0.7)\n",
    "    - 문맥과 의미를 더 중요하게 고려\n",
    "    - 유사한 주제의 문서도 검색 가능\n",
    "    - 예: \"AI 기술의 미래 전망\" → AI 발전 방향, 기술 트렌드 등 관련 문서 포함\n",
    "\n",
    "- 키워드 검색 중심 (semantic_weight=0.3)\n",
    "    - 정확한 키워드 매칭을 중시\n",
    "    - 특정 용어나 개념이 포함된 문서 우선\n",
    "    - 예: \"삼성전자 AI 칩\" → 정확히 해당 키워드가 포함된 문서 우선\n",
    "\n",
    "- 균형잡힌 검색 (semantic_weight=0.5)\n",
    "    - 두 방식의 장점을 균형있게 활용\n",
    "    - 일반적인 검색에 적합\n",
    "    - 예: \"자율주행 안전\" → 키워드 매칭과 의미적 연관성 모두 고려"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from typing import List, Dict, Tuple\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_EMBEDDING_MODEL = os.getenv(\"OPENAI_EMBEDDING_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIBooksRAG:\n",
    "    \"\"\"\n",
    "    JSON 데이터를 활용한 Hybrid RAG 시스템\n",
    "    - LangChain의 FAISS 벡터스토어를 이용한 의미론적 검색\n",
    "    - BM25를 이용한 키워드 기반 검색\n",
    "    - 하이브리드 검색 기능 제공\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.vector_store = None\n",
    "        self.metadata = []\n",
    "        self.bm25 = None\n",
    "        self.embeddings = OpenAIEmbeddings(model=OPENAI_EMBEDDING_MODEL, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "    # 1. 임베딩 생성 함수\n",
    "    def get_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"텍스트를 OpenAI 임베딩 모델로 임베딩\"\"\"\n",
    "        return self.embeddings.embed_query(text)\n",
    "\n",
    "    # 2. 목차 파싱 함수\n",
    "    def parse_toc(self, toc_html: str) -> List[Dict[str, List[str]]]:\n",
    "        \"\"\"HTML 형태의 목차 데이터를 파싱하여 계층적 구조로 반환\"\"\"\n",
    "        soup = BeautifulSoup(toc_html, \"html.parser\")\n",
    "        chapters = [b.get_text() for b in soup.find_all(\"b\")]\n",
    "        items = [br.next_sibling.strip() for br in soup.find_all(\"br\") if br.next_sibling]\n",
    "\n",
    "        structured_toc = []\n",
    "        current_chapter = None\n",
    "\n",
    "        for item in items:\n",
    "            if item in chapters:\n",
    "                current_chapter = item\n",
    "                structured_toc.append({\"chapter\": current_chapter, \"items\": []})\n",
    "            elif current_chapter:\n",
    "                structured_toc[-1][\"items\"].append(item)\n",
    "\n",
    "        return structured_toc\n",
    "\n",
    "    # 3. 데이터 임베딩 및 문서 생성 함수\n",
    "    def create_documents(self, book_data: List) -> List[Document]:\n",
    "        \"\"\"책 데이터를 LangChain Document로 변환\"\"\"\n",
    "        documents = []\n",
    "        \n",
    "        for book in book_data:\n",
    "            # 책 기본 정보\n",
    "            documents.append(Document(\n",
    "                page_content=book[\"title\"],\n",
    "                metadata={\n",
    "                    \"type\": \"title\",\n",
    "                    \"author\": book[\"author\"],\n",
    "                    \"pubDate\": book[\"pubDate\"],\n",
    "                    \"categoryName\": book[\"categoryName\"]\n",
    "                }\n",
    "            ))\n",
    "            documents.append(Document(\n",
    "                page_content=book[\"description\"],\n",
    "                metadata={\n",
    "                    \"type\": \"description\",\n",
    "                    \"author\": book[\"author\"],\n",
    "                    \"pubDate\": book[\"pubDate\"],\n",
    "                    \"categoryName\": book[\"categoryName\"]\n",
    "                }\n",
    "            ))\n",
    "\n",
    "            # 목차 정보\n",
    "            toc_html = book[\"toc\"]\n",
    "            structured_toc = self.parse_toc(toc_html)\n",
    "            for chapter in structured_toc:\n",
    "                for item in chapter[\"items\"]:\n",
    "                    item_with_context = f\"{chapter['chapter']} - {item}\"\n",
    "                    documents.append(Document(\n",
    "                        page_content=item_with_context,\n",
    "                        metadata={\n",
    "                            \"type\": \"toc\",\n",
    "                            \"chapter\": chapter[\"chapter\"],\n",
    "                            \"item\": item,\n",
    "                            \"author\": book[\"author\"],\n",
    "                            \"pubDate\": book[\"pubDate\"],\n",
    "                            \"categoryName\": book[\"categoryName\"]\n",
    "                        }\n",
    "                    ))\n",
    "        return documents\n",
    "\n",
    "    # 4. JSON 데이터 로드 함수\n",
    "    def load_json_files(self, directory: str) -> List[Dict]:\n",
    "        \"\"\"지정된 디렉토리에서 모든 JSON 파일을 읽어 책 정보 리스트 반환\"\"\"\n",
    "        data = []\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith(\".json\"):\n",
    "                filepath = os.path.join(directory, filename)\n",
    "                with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data.append(json.load(f))\n",
    "        return data\n",
    "\n",
    "    # 5. FAISS 벡터스토어 생성\n",
    "    def create_vector_store(self, json_dir: str, vector_store_path: str):\n",
    "        \"\"\"JSON 데이터를 읽고 FAISS 벡터스토어 생성 및 저장\"\"\"\n",
    "        book_data_list = self.load_json_files(json_dir)\n",
    "        documents = []\n",
    "        for book_data in book_data_list:\n",
    "            documents.extend(self.create_documents(book_data))\n",
    "\n",
    "        # LangChain FAISS 벡터스토어 생성\n",
    "        vector_store = FAISS.from_documents(documents, self.embeddings)\n",
    "        vector_store.save_local(vector_store_path)\n",
    "        self.vector_store = vector_store\n",
    "        self.metadata = documents\n",
    "\n",
    "    # 6. FAISS 벡터스토어 로드\n",
    "    def load_vector_store(self, vector_store_path: str):\n",
    "        \"\"\"FAISS 벡터스토어 및 메타데이터 로드\"\"\"\n",
    "        self.vector_store = FAISS.load_local(\n",
    "            vector_store_path,\n",
    "            embeddings=self.embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        self.metadata = [doc.metadata for doc in self.vector_store.similarity_search(\"\", k=5)]\n",
    "\n",
    "    # 7. BM25 초기화\n",
    "    def initialize_bm25(self):\n",
    "        \"\"\"BM25 검색 엔진 초기화\"\"\"\n",
    "        tokenized_corpus = [doc.page_content.lower().split() for doc in self.metadata]\n",
    "        self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "    # 8. Hybrid 검색\n",
    "    def hybrid_search(self, query: str, k: int = 5, semantic_weight: float = 0.5) -> List[Tuple[str, float]]:\n",
    "        \"\"\"FAISS 및 BM25를 결합한 하이브리드 검색\"\"\"\n",
    "        # FAISS 검색\n",
    "        faiss_results = self.vector_store.similarity_search_with_score(query, k=k)\n",
    "\n",
    "        # BM25 검색\n",
    "        tokenized_query = query.lower().split()\n",
    "        bm25_scores = self.bm25.get_scores(tokenized_query)\n",
    "        bm25_results = sorted(\n",
    "            [(self.metadata[i], bm25_scores[i]) for i in range(len(self.metadata))],\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )[:k]\n",
    "\n",
    "        # 하이브리드 결합\n",
    "        combined_scores = {}\n",
    "        for doc, score in faiss_results:\n",
    "            combined_scores[doc.metadata[\"type\"]] = semantic_weight * (1 - score)\n",
    "        for doc, score in bm25_results:\n",
    "            if doc[\"type\"] in combined_scores:\n",
    "                combined_scores[doc[\"type\"]] += (1 - semantic_weight) * score\n",
    "            else:\n",
    "                combined_scores[doc[\"type\"]] = (1 - semantic_weight) * score\n",
    "\n",
    "        sorted_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return sorted_results[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m rag\u001b[38;5;241m.\u001b[39mload_vector_store(vector_store_path)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# BM25 초기화\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m rag\u001b[38;5;241m.\u001b[39minitialize_bm25()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 하이브리드 검색\u001b[39;00m\n\u001b[0;32m     18\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m고등학교참고서\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[52], line 122\u001b[0m, in \u001b[0;36mAIBooksRAG.initialize_bm25\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitialize_bm25\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    121\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"BM25 검색 엔진 초기화\"\"\"\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     tokenized_corpus \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata]\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbm25 \u001b[38;5;241m=\u001b[39m BM25Okapi(tokenized_corpus)\n",
      "Cell \u001b[1;32mIn[52], line 122\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitialize_bm25\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    121\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"BM25 검색 엔진 초기화\"\"\"\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     tokenized_corpus \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata]\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbm25 \u001b[38;5;241m=\u001b[39m BM25Okapi(tokenized_corpus)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "# AIBooksRAG 클래스 초기화\n",
    "rag = AIBooksRAG()\n",
    "\n",
    "# JSON 데이터 디렉토리 및 저장 경로\n",
    "json_dir = \"./books\"\n",
    "vector_store_path = \"./books_vectorstore\"\n",
    "\n",
    "# 벡터스토어 생성\n",
    "rag.create_vector_store(json_dir, vector_store_path)\n",
    "\n",
    "# 벡터스토어 로드\n",
    "rag.load_vector_store(vector_store_path)\n",
    "\n",
    "# BM25 초기화\n",
    "rag.initialize_bm25()\n",
    "\n",
    "# 하이브리드 검색\n",
    "query = \"고등학교참고서\"\n",
    "results = rag.hybrid_search(query, k=5)\n",
    "print(\"\\n=== Hybrid Search Results ===\")\n",
    "for doc_type, score in results:\n",
    "    print(f\"Type: {doc_type}, Score: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 API 호출 함수\n",
    "def get_embedding(text: str, model: str = OPENAI_EMBEDDING_MODEL) -> List[float]:\n",
    "    \"\"\"텍스트를 주어진 임베딩 모델로 임베딩\"\"\"\n",
    "    response = requests.post(\n",
    "        \"https://api.openai.com/v1/embeddings\",\n",
    "        headers={\"Authorization\": f\"Bearer {OPENAI_API_KEY}\"},\n",
    "        json={\"model\": model, \"input\": text},\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"data\"][0][\"embedding\"]\n",
    "\n",
    "# 목차 데이터 파싱 함수\n",
    "def parse_toc(toc_html: str) -> List[Dict[str, List[str]]]:\n",
    "    \"\"\"HTML 형태의 목차 데이터를 파싱하여 계층적 구조로 반환\"\"\"\n",
    "    soup = BeautifulSoup(toc_html, \"html.parser\")\n",
    "    chapters = [b.get_text() for b in soup.find_all(\"b\")]\n",
    "    items = [br.next_sibling.strip() for br in soup.find_all(\"br\") if br.next_sibling]\n",
    "\n",
    "    structured_toc = []\n",
    "    current_chapter = None\n",
    "\n",
    "    for item in items:\n",
    "        # 큰 단원이 포함된 경우\n",
    "        if item in chapters:\n",
    "            current_chapter = item\n",
    "            structured_toc.append({\"chapter\": current_chapter, \"items\": []})\n",
    "        elif current_chapter:\n",
    "            structured_toc[-1][\"items\"].append(item)\n",
    "\n",
    "    return structured_toc\n",
    "\n",
    "# 데이터 임베딩 함수\n",
    "def embed_book_data(book_data: Dict, model: str = OPENAI_EMBEDDING_MODEL):\n",
    "    \"\"\"책 데이터를 계층적으로 임베딩\"\"\"\n",
    "    # 책 기본 정보 임베딩\n",
    "    title_embedding = get_embedding(book_data[\"title\"], model=model)\n",
    "    description_embedding = get_embedding(book_data[\"description\"], model=model)\n",
    "    category_embedding = get_embedding(book_data[\"categoryName\"], model=model)\n",
    "\n",
    "    # 목차 임베딩\n",
    "    toc_html = book_data[\"toc\"]\n",
    "    structured_toc = parse_toc(toc_html)\n",
    "\n",
    "    toc_embeddings = []\n",
    "    for chapter in structured_toc:\n",
    "        chapter_title = chapter[\"chapter\"]\n",
    "        for item in chapter[\"items\"]:\n",
    "            # 문맥 포함하여 임베딩 생성\n",
    "            item_with_context = f\"{chapter_title} - {item}\"\n",
    "            item_embedding = get_embedding(item_with_context, model=model)\n",
    "            toc_embeddings.append({\n",
    "                \"chapter\": chapter_title,\n",
    "                \"item\": item,\n",
    "                \"embedding\": item_embedding\n",
    "            })\n",
    "\n",
    "    return {\n",
    "        \"title_embedding\": title_embedding,\n",
    "        \"description_embedding\": description_embedding,\n",
    "        \"category_embedding\": category_embedding,\n",
    "        \"toc_embeddings\": toc_embeddings,\n",
    "    }\n",
    "\n",
    "# JSON 파일 읽기 함수\n",
    "def load_json_files(directory: str) -> List[Dict]:\n",
    "    \"\"\"지정된 디렉토리에서 모든 JSON 파일을 읽어 책 정보 리스트 반환\"\"\"\n",
    "    data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".json\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                data.append(json.load(f))\n",
    "    return data\n",
    "\n",
    "# FAISS 인덱스 생성 및 저장\n",
    "def create_faiss_vectorstore(json_dir: str, faiss_index_path: str, metadata_path: str):\n",
    "    \"\"\"\n",
    "    JSON 데이터를 읽고 FAISS 인덱스와 메타데이터 저장\n",
    "    \"\"\"\n",
    "    # JSON 파일 로드\n",
    "    book_data_list = load_json_files(json_dir)[0]\n",
    "\n",
    "    # 임베딩과 메타데이터 생성\n",
    "    all_vectors = []\n",
    "    metadata = []\n",
    "\n",
    "    for book_data in book_data_list:\n",
    "        embeddings = embed_book_data(book_data)\n",
    "        # 벡터 추가\n",
    "        all_vectors.append(embeddings[\"title_embedding\"])\n",
    "        all_vectors.append(embeddings[\"description_embedding\"])\n",
    "        all_vectors.append(embeddings[\"category_embedding\"])\n",
    "\n",
    "        # 메타데이터 추가\n",
    "        metadata.append({\n",
    "            \"title\": book_data[\"title\"],\n",
    "            \"author\": book_data[\"author\"],\n",
    "            \"pubDate\": book_data[\"pubDate\"],\n",
    "            \"categoryName\": book_data[\"categoryName\"]\n",
    "        })\n",
    "\n",
    "        # 목차 벡터 추가\n",
    "        for toc in embeddings[\"toc_embeddings\"]:\n",
    "            all_vectors.append(toc[\"embedding\"])\n",
    "            metadata.append({\n",
    "                \"title\": book_data[\"title\"],\n",
    "                \"chapter\": toc[\"chapter\"],\n",
    "                \"item\": toc[\"item\"]\n",
    "            })\n",
    "\n",
    "    # NumPy 배열로 변환\n",
    "    all_vectors = np.array(all_vectors, dtype=np.float32)\n",
    "\n",
    "    # FAISS 인덱스 생성\n",
    "    dimension = all_vectors.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(all_vectors)\n",
    "\n",
    "    # 저장\n",
    "    faiss.write_index(index, faiss_index_path)\n",
    "    print(f\"FAISS 인덱스 저장 완료: {faiss_index_path}\")\n",
    "\n",
    "    with open(metadata_path, \"wb\") as f:\n",
    "        pickle.dump(metadata, f)\n",
    "    print(f\"메타데이터 저장 완료: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIBooksRAG:\n",
    "    \"\"\"\n",
    "    AI 책 검색을 위한 RAG(Retrieval-Augmented Generation) 시스템\n",
    "    \n",
    "    이 클래스는 의미론적 검색과 키워드 기반 검색을 결합한 하이브리드 검색 기능을 제공합니다.\n",
    "\n",
    "    Attributes:\n",
    "        embeddings: OpenAI 임베딩 모델\n",
    "        vector_store: FAISS 벡터 저장소\n",
    "        bm25: 키워드 기반 검색을 위한 BM25 모델\n",
    "        doc_mapping: 문서 ID와 문서 객체 간의 매핑\n",
    "        logger: 로깅을 위한 로거 객체\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_model):\n",
    "        self.embeddings = embedding_model\n",
    "        self.vector_store = None\n",
    "        self.bm25 = None\n",
    "        self.doc_mapping = None\n",
    "        self.logger = logger\n",
    "\n",
    "    def create_vector_store(self, documents: List[Document], vector_store_path: str):\n",
    "        \"\"\"\n",
    "        문서로부터 FAISS 벡터스토어를 생성.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"FAISS 벡터스토어 생성 중...\")\n",
    "        \n",
    "        # 문서 임베딩\n",
    "        embeddings = np.array([self.embeddings.embed_query(doc.page_content) for doc in documents], dtype=\"float32\")\n",
    "        \n",
    "        # FAISS 인덱스 생성\n",
    "        dimension = embeddings.shape[1]\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(embeddings)\n",
    "        \n",
    "        # 저장\n",
    "        os.makedirs(vector_store_path, exist_ok=True)\n",
    "        faiss.write_index(index, os.path.join(vector_store_path, \"index.faiss\"))\n",
    "        \n",
    "        # 문서 매핑 저장\n",
    "        with open(os.path.join(vector_store_path, \"metadata.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(documents, f)\n",
    "        \n",
    "        self.logger.info(\"FAISS 벡터스토어 생성 완료.\")\n",
    "        self.vector_store = index\n",
    "        self.doc_mapping = {i: doc for i, doc in enumerate(documents)}\n",
    "\n",
    "    def load_vector_store(self, vector_store_path: str):\n",
    "        \"\"\"\n",
    "        저장된 FAISS 벡터스토어와 문서 매핑 로드.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"FAISS 벡터스토어를 {vector_store_path}에서 로드합니다...\")\n",
    "            self.vector_store = faiss.read_index(os.path.join(vector_store_path, \"index.faiss\"))\n",
    "            \n",
    "            with open(os.path.join(vector_store_path, \"metadata.pkl\"), \"rb\") as f:\n",
    "                documents = pickle.load(f)\n",
    "                self.doc_mapping = {i: doc for i, doc in enumerate(documents)}\n",
    "            \n",
    "            self.logger.info(\"FAISS 벡터스토어 로드 완료.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"로드 실패: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def initialize_bm25(self, documents: List[Document]):\n",
    "        \"\"\"\n",
    "        BM25 검색 엔진 초기화.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"BM25 검색 엔진 초기화 중...\")\n",
    "        tokenized_corpus = [doc.page_content.lower().split() for doc in documents]\n",
    "        self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "        self.logger.info(\"BM25 검색 엔진 초기화 완료.\")\n",
    "\n",
    "    def semantic_search(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"\n",
    "        FAISS 벡터스토어를 활용한 의미론적 검색.\n",
    "        \"\"\"\n",
    "        query_embedding = np.array([self.embeddings.embed_query(query)], dtype=\"float32\")\n",
    "        distances, indices = self.vector_store.search(query_embedding, k)\n",
    "        results = [(self.doc_mapping[idx], distances[0][i]) for i, idx in enumerate(indices[0])]\n",
    "        return results\n",
    "\n",
    "    def keyword_search(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"\n",
    "        BM25를 활용한 키워드 검색.\n",
    "        \"\"\"\n",
    "        if not self.bm25:\n",
    "            raise ValueError(\"BM25 검색 엔진이 초기화되지 않았습니다.\")\n",
    "        \n",
    "        tokenized_query = query.lower().split()\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        top_k_indices = np.argsort(scores)[-k:][::-1]\n",
    "        results = [(self.doc_mapping[idx], scores[idx]) for idx in top_k_indices]\n",
    "        return results\n",
    "\n",
    "    def hybrid_search(self, query: str, k: int = 5, semantic_weight: float = 0.5) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"\n",
    "        의미론적 검색과 키워드 검색을 결합한 하이브리드 검색.\n",
    "        \"\"\"\n",
    "        semantic_results = self.semantic_search(query, k)\n",
    "        keyword_results = self.keyword_search(query, k)\n",
    "        \n",
    "        combined_scores = {}\n",
    "        \n",
    "        for doc, score in semantic_results:\n",
    "            doc_id = id(doc)\n",
    "            combined_scores[doc_id] = {'doc': doc, 'score': semantic_weight * (1 - score)}\n",
    "        \n",
    "        for doc, score in keyword_results:\n",
    "            doc_id = id(doc)\n",
    "            if doc_id in combined_scores:\n",
    "                combined_scores[doc_id]['score'] += (1 - semantic_weight) * score\n",
    "            else:\n",
    "                combined_scores[doc_id] = {'doc': doc, 'score': (1 - semantic_weight) * score}\n",
    "        \n",
    "        sorted_results = sorted(\n",
    "            [(info['doc'], info['score']) for info in combined_scores.values()],\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )[:k]\n",
    "        \n",
    "        return sorted_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-13 09:59:57,561 - 데이터를 books_vectorstore에서 로드합니다...\n",
      "2025-01-13 09:59:57,563 - 로드 중 오류 발생: too many values to unpack (expected 2)\n",
      "벡터 스토어 로드 실패: too many values to unpack (expected 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m rag \u001b[38;5;241m=\u001b[39m AIBooksRAG(embedding_model)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# 기존 벡터 스토어 로드 시도\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m     rag\u001b[38;5;241m.\u001b[39mload_vector_store(vector_store_path, metadata_path)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ 기존 벡터 스토어를 로드했습니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[24], line 157\u001b[0m, in \u001b[0;36mAIBooksRAG.load_vector_store\u001b[1;34m(self, vector_store_path, processed_docs_path)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m데이터를 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvector_store_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m에서 로드합니다...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# 벡터 스토어 로드\u001b[39;00m\n\u001b[1;32m--> 157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_store \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mload_local(\n\u001b[0;32m    158\u001b[0m     folder_path\u001b[38;5;241m=\u001b[39mvector_store_path,\n\u001b[0;32m    159\u001b[0m     embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings,\n\u001b[0;32m    160\u001b[0m     allow_dangerous_deserialization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    161\u001b[0m )\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m벡터스토어 로드 완료\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# processed_docs 로드\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\SpartaProjects\\Group_Project\\CBook\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1207\u001b[0m, in \u001b[0;36mFAISS.load_local\u001b[1;34m(cls, folder_path, embeddings, index_name, allow_dangerous_deserialization, **kwargs)\u001b[0m\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;66;03m# load docstore and index_to_docstore_id\u001b[39;00m\n\u001b[0;32m   1206\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m-> 1207\u001b[0m     (\n\u001b[0;32m   1208\u001b[0m         docstore,\n\u001b[0;32m   1209\u001b[0m         index_to_docstore_id,\n\u001b[0;32m   1210\u001b[0m     ) \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(  \u001b[38;5;66;03m# ignore[pickle]: explicit-opt-in\u001b[39;00m\n\u001b[0;32m   1211\u001b[0m         f\n\u001b[0;32m   1212\u001b[0m     )\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(embeddings, index, docstore, index_to_docstore_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# 환경 변수에서 경로 가져오기\n",
    "vector_store_path = \"books_vectorstore\"\n",
    "books_dir = \"books\"\n",
    "metadata_path = \"books_vectorstore/index.pkl\"\n",
    "\n",
    "# 임베딩 모델 초기화 \n",
    "embedding_model = OpenAIEmbeddings(model=OPENAI_EMBEDDING_MODEL)\n",
    "\n",
    "# RAG 시스템 초기화\n",
    "rag = AIBooksRAG(embedding_model)\n",
    "\n",
    "try:\n",
    "    # 기존 벡터 스토어 로드 시도\n",
    "    rag.load_vector_store(vector_store_path, metadata_path)\n",
    "    print(\"✅ 기존 벡터 스토어를 로드했습니다.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"벡터 스토어 로드 실패: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# 대화형 검색 시작\n",
    "print(\"\\n🔍 AI 뉴스 검색 시스템을 시작합니다.\")\n",
    "print(\"- 종료하려면 'q' 또는 'quit'를 입력하세요.\")\n",
    "\n",
    "search_mode = \"hybrid\" # 검색 방식 변경은 'mode [semantic/keyword/hybrid]'를 입력하세요.\n",
    "while True:\n",
    "    query = input(\"\\n🔍 검색할 내용을 입력하세요: \").strip()\n",
    "\n",
    "    if not query:\n",
    "        continue\n",
    "        \n",
    "    if query.lower() in ['q', 'quit']:\n",
    "        print(\"\\n👋 검색을 종료합니다.\")\n",
    "        break\n",
    "        \n",
    "    if query.lower().startswith('mode '):\n",
    "        mode = query.split()[1].lower()\n",
    "        if mode in ['semantic', 'keyword', 'hybrid']:\n",
    "            search_mode = mode\n",
    "            print(f\"\\n✅ 검색 모드를 '{mode}'로 변경했습니다.\")\n",
    "        else:\n",
    "            print(\"\\n❌ 잘못된 검색 모드입니다. semantic/keyword/hybrid 중 선택하세요.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        print(f\"\\n'{query}' 검색을 시작합니다... (모드: {search_mode})\")\n",
    "        \n",
    "        if search_mode == \"hybrid\":\n",
    "            results = rag.hybrid_search(query, k=5, semantic_weight=0.5)\n",
    "        elif search_mode == \"semantic\":\n",
    "            results = rag.vector_store.similarity_search_with_score(query, k=5)\n",
    "        else:  # keyword\n",
    "            results = rag.keyword_search(query, k=5)\n",
    "        \n",
    "        print(f\"\\n✨ 검색 완료! {len(results)}개의 결과를 찾았습니다.\\n\")\n",
    "        \n",
    "        # 결과 출력\n",
    "        for i, result in enumerate(results, 1):\n",
    "            doc = result[0]\n",
    "            score = result[1]\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"검색 결과 {i}/{len(results)}\")\n",
    "            print(f\"제목: {doc.metadata['title']}\")\n",
    "            print(f\"날짜: {doc.metadata['date']}\")\n",
    "            if search_mode == \"hybrid\":\n",
    "                print(f\"통합 점수: {score:.4f}\")\n",
    "            elif search_mode == \"semantic\":\n",
    "                print(f\"유사도 점수: {1 - (score/2):.4f}\")\n",
    "            else:\n",
    "                print(f\"BM25 점수: {score:.4f}\")\n",
    "            print(f\"URL: {doc.metadata['url']}\")\n",
    "            print(f\"{'-'*40}\")\n",
    "            print(f\"내용:\\n{doc.page_content[:300]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ 검색 중 오류가 발생했습니다: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CBook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
